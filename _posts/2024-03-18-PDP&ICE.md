---
title: 'Introduction to ICE and PDP Curves'
date: 2023-03-18
permalink: /posts/2023/03/PDP-ICE/
tags:
  - Post-hoc Explainers
  - PDP
  - ICE
---

Let's understand together what are Partial Dependence Plots (PDP) and Individual Conditional Expectations (ICE) curves.
We will also see how to compute these quantities using the [PyFD](https://github.com/gablabc/PyFD) Python library.

## Setup

We first set up a toy experiment involving two features $x_0$ and $x_1$ which are sampled uniformly between $-1$ and $1$.
The model we wish to characterize is the following $h(x) = x_0\,x_1$.

```python
from pyfd.features import Features

np.random.seed(42)
N = 500
X = np.random.uniform(-1, 1, size=(N, 2))
feature_names = ["x0", "x1"]
feature_types = ["num", "num"]
features = Features(X, feature_names, feature_types)
def h(X):
    return X[:, 0] * X[:, 1]
```

This function may seem simplistic, but it is already hard to explain in a model-agnostic way.

## ICE & PDP Curves

Our task is to understand the effect of each feature on the model response without having any access to the model's
inner structure. We are only able to query the model at input $x$ and collect the output $h(x)$. How then would
you quantify the impact of varying feature $x_k$ on the model? The solution proposed by
[(Goldstein et al, 2015)](https://arxiv.org/abs/1309.6392) is to freeze $x_{-k}$ at a value $z_{-k}$ from the dataset
and then vary $x_k$. This leads to the so-called Individual Conditional Expectations (ICE) curves

$$\text{ICE}(x_k) := h(x_k,z_{-k}).$$

In `PyFD` we use a slightly different definition where we vertically shift the curve so that it passes by $0$ at
$x_k=z_k$ (the reference point).

$$\text{ICE}(x_k) := h(x_k, z_{-k}) - h(z).$$

The theoretical reasons behind this choice go beyond the scope of this blog post. Just note that we can really add any constant
to a ICE curve without impacting its interpretation. Indeed, it is the **slope** of the curve that is interesting
*e.g.* the curve goes up/down or is flat. Here is how to compute ICE curves via `PyFD`

```python
from pyfd.decompositions import get_components_brute_force

# Get the ice curves
ice = get_components_brute_force(h, X, X, anchored=True)
print(ice.keys())
>>>dict_keys([(), (0,), (1,)])
print(ice[(0,)].shape)
>>> (500, 500)
```

The primitive `get_components_brute_force` returns a `dict` whose keys
represent the different ICE curves. For instance, `h_components[(k,)]`
returns a numpy array of shape `(N,N)` whose element `[i,j]` represents
$$h(x_k^{(i)},x^{(j)}_{-k}) - h(x^{(j)})$$ which is the ICE curve of feature $k$ evaluated at $x_k^{(i)}$ while the
remaining features are frozen at $x^{(j)}_{-k}$. These curves can be plotted as follows.

```python
from pyfd.plots import partial_dependence_plot

partial_dependence_plot(ice, X, X, features, centered=False, alpha=0.05)
plt.show()
```

![ice](/images/blog-bb/ice.png)

Each of these gray curve is a ICE for a given $z_{-k}$ from the
dataset. We can therefore see the impact of varying $x_k$ while the remaining
feature are fixed. Note that some ICE curves go up and others go down.
Mathematically speaking, we say that features $x_0$ and $x_1$ **interact**.

> Two features interact if and only if the effect of varying one depends on the other.

It makes sense to see ICE curves going up and down given that $h(x)=x_0\,x_1$. Logically, when $x_0$ is positive,
the model should have a positive slope w.r.t $x_1$, and a negative slope when $x_0$ is negative.
Nonetheless, remember that we are in a model-agnostic setting where we must discover these patterns by querying the model.
Thus, these experimental results are not so trivial.

ICE curves are very simple to comprehend and implement, but they have major issues. For example, it becomes hard to disentangle them
visually, when there are many samples of data. I challenge you to choose one gray curve and follow it visually from left to right without
confusing it with another curve. In this example is it not so impossible because the lines are straight, but imagine what will happen
when we have a huge spaghetti of wiggly curves.

![ice](/images/blog-bb/spaguetti.png)

To simplify the visualization, we also plotted the **average** ICE curve in black. This curve is referred to as the
Partial Dependence Plot (PDP) [(Friedman, 2001)](https://www.jstor.org/stable/2699986). Its definition is

$$\text{PDP}(x_k) = \mathbb{E}_{z\sim\mathcal{B}}[h(x_k,z_{-k}) - h(z)].$$

PDPs are a useful alternative when there are too many ICE curves to visualize. Yet PDPs have issues of their own :
they can be misleading when some ICE curves go up and others go down. In the example above for instance, the PDP curves are
essentially flat, which could be misinterpreted as stating that $h$ does not rely on $x_0$ nor $x_1$.
This is outrageously false!

## Regional PDP/ICE curves

We need a compromise between the readability of PDPs and the faithfulness of ICE. The solution proposed by the tool
GADGET-PDP [(Herbinger et al., 2023)](https://arxiv.org/abs/2306.00541) it to discover disjoint regions on the input space
$$\mathcal{X}_{-k}$$ of remaining features. For each separate region, we compute the ICE/PDP curves $\text{ICE}(x_k)$
and $\text{PDP}(x_k)$ while only freezing $x_{-k}$ at values $z_{-k}$ that land in said region.
The most user-friendly way to run GADGET-PDP in `PyFD` is to pass `groups_method='gadget-pdp'` as argument to `partial_dependence_plot`.

```python
partial_dependence_plot(ice, X, X, features, 
                        groups_method='gadget-pdp',
                        fd_trees_kwargs = {'alpha': 0.05, 'max_depth':2},
                        centered=False, alpha=0.05)
plt.show()
```

![ice](/images/blog-bb/gadget_pdp.png)

This plot allows us to understand the relationship between the slope of $\text{ICE}(x_k)$ and the frozen values of remaining feature.
The magic part is that the regions provided by GADGET-PDP are **interpretable** and are fitted **automatically** on your data.
You don't have to find these regions yourself.

## Takeaways

In this blog we have learned that

1. ICE curves show the effect of varying a feature while keeping the remaining ones frozen at a value from the dataset.
2. PDP are the average ICE curve and so they convey the *average effect of varying feature $k$*.
3. ICE curves can be hard to interpret because of their large amount, while PDP can provide misleading interpretations.
A compromise between readability and faithfulness is provided by Regional PDP curves.
